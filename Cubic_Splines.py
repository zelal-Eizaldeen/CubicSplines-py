# -*- coding: utf-8 -*-
"""CS598_Coding_Assignment_3 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FIZQAbE4u1ztn_kSQYn92BT46f54RdY

# **CS598 Coding Assignment 3**

# **Team Members**

Zilal Eiz Aldin (Online MCS) Zelalae2@illinois.edu

Mesay Taye (Online MCS) mesayst2@illinois.edu

Jianci Zhai (Online MCS) jianciz2@illinois.edu

# **Contribution of each member**

Part3 By Mesay

Part2 By Zilal

Part1 By Jianci

**Reference**

https://campuswire.com/c/GB46E5679/feed/337

Ridgless: https://campuswire.com/c/GB46E5679/feed/338

# **Basic Setting**
"""

import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


from scipy import stats
from scipy.optimize import minimize_scalar

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression as lm
from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

from google.colab import drive
drive.mount('/content/drive')

import seaborn as sns

import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.preprocessing import SplineTransformer
from sklearn.cluster import KMeans



np.random.seed(3113)

"""##Part 1 By Zilal"""

!pip install scikit-fda
!pip install scikit-misc
!pip install scikit-misc==0.3.1
!pip install csaps

"""Recall: y_hat=S*y where y_hat has nx1 and y the same
S=F(F_transpose * F + lam * Omega) * F_transpose
- S depends on x not on y
y_hat is the first column of S matrix
"""

from skmisc.loess import loess
from csaps import csaps

## Get the S matrix
def S_lam(x, lam):
    n = len(x)
    Y = np.identity(n) #because S depends only on x so this is a fake_y
    A = smooth(x, Y, x, lam, axis=0)
    return (A + A.T)/2

'''
the input of this function can only be x and span, not y
'''

def sm_diag(x,span):
  from skmisc.loess import loess
  lowess_model = loess(x.flatten(), x.flatten(),span = span)
  lowess_model.fit()
  return lowess_model.outputs.diagonal.reshape(-1,1)

# fit a smoother spline which will return smooth which is the degree of freedom
# smooth method will calculate lamda and calculate Smoother matrix S and do Trace of(Smoother matrix) which is df
def smooth(x, y, fx, lam, axis=-1):
    return csaps(x, y, fx, smooth=1/(lam + 1), axis=-1)
# Get lambda
def df_to_lam(x, df):
    def get_lam(lam):
        return (S_lam(x, lam).trace() - df)**2
    return minimize_scalar(get_lam, bounds=(0, 10), options={'xatol': 1e-10}).x

def smooth_df(x, y, fx, df, axis=-1):
    return smooth(x, y, fx, df_to_lam(x, df), axis)

# Get Smoother matrix
def S_df(x, df):
    return S_lam(x, df_to_lam(x,  df))

#Use GCV, LCV to choose lambda
def gcv(x, y, df):
    n=len(x)
    fit = smooth_df(x, y, x, df)
    return (((y - fit) ** 2) / ((1 - df/n) ** 2)).mean()

def lcv(x, y, df):

    fit = smooth_df(x, y, x, df)
    lev = np.diag(S_df(x, df=df))
    return (((y - fit) ** 2) / ((1 - lev) ** 2)).mean()

"""Recall: y_hat = x(x_transpose*x)^-1 * x_transpose * y
where x(x_transpose*x)^-1 called projection matrix for regression model
"""

# Example usage
coding3_data = pd.read_csv("https://liangfgithub.github.io/Data/Coding3_Data.csv")

coding3_data.head()

span_range = np.around(np.linspace(0.2, 0.9, 15),2)
x = coding3_data['x'].values
y = coding3_data['y'].values



dfs = 2+np.arange(1, 15)/2
m = len(dfs)
my_lcv = np.zeros(m)
my_gcv = np.zeros(m)
for i in range(m):
    df = dfs[i]
    my_gcv[i] = gcv(x, y, df)
    my_lcv[i] = lcv(x, y, df)
plt.plot(dfs, my_lcv, color='red')
plt.scatter(dfs, my_lcv, color='red', marker='d', s=10, label='LOO CV')
plt.plot(dfs, my_gcv, color='blue')
plt.scatter(dfs, my_gcv, color='blue', marker='s', s=10, label='GCV')
plt.legend()
plt.xlabel("df")
plt.ylabel("CV Score")

opt_df = dfs[np.argmin(my_gcv)]
print(opt_df)

def f(x):
    return np.sin(12 * (x + 0.2)) / (x + 0.2)


np.random.seed(3113)
n = 30
err = 1
x = np.sort(np.random.uniform(size=n))
y = f(x) + np.random.normal(0, err, n)
fx = np.arange(1, 51, 1)/50
fy = f(fx)

plt.scatter(x, y, color="red", s=2)
plt.plot(fx, fy, color="gray", linewidth=1, label="True Function")
plt.plot(np.arange(1, 101)/100, smooth_df(x, y, np.arange(1, 101)/100, opt_df), color="blue",
         linestyle="dashed", linewidth=1, label="Fitted Line")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()



"""# **Part I: Optimal span for LOESS**"""

!pip install scikit-misc==0.3.1 #match the exact version from the reference, function output keywords slightly different

!pip install csaps

from skmisc.loess import loess
from csaps import csaps

coding3_data = pd.read_csv("https://liangfgithub.github.io/Data/Coding3_Data.csv")

"""Task 1. Write a function to retrieve the Diagonal of the Smoother Matrix

reference:
https://has2k1.github.io/scikit-misc/stable/generated/skmisc.loess.loess_outputs.html#skmisc.loess.loess_outputs
"""

'''
the input of this function can only be x and span, not y
'''

def sm_diag(x,span):
  from skmisc.loess import loess
  lowess_model = loess(x.flatten(), x.flatten(),span = span)
  lowess_model.fit()
  return lowess_model.outputs.diagonal.reshape(-1,1)

"""Task 2. Write a function to find the Optimal Span(s) based on CV and GCV.


• Iterate over the specified span values.

• For each span, calculate the CV and GCV values.

• Return the CV and GCV values corresponding to each span.
"""

def smooth(x, y, fx, lam, axis=-1):
    return csaps(x, y, fx, smooth=1/(lam + 1), axis=-1)

def S_lam(x, lam):
    n = len(x)
    Y = np.identity(n)
    A = smooth(x, Y, x, lam, axis=0)
    return (A + A.T)/2


def df_to_lam(x, df):
    def g(lam):
        return (S_lam(x, lam).trace() - df)**2
    return minimize_scalar(g, bounds=(0, 10), options={'xatol': 1e-10}).x


def smooth_df(x, y, fx, df, axis=-1):
    return smooth(x, y, fx, df_to_lam(x, df), axis)


def S_df(x, df):
    return S_lam(x, df_to_lam(x,  df))

def gcv(x, y, df):
    n=len(x)
    fit = smooth_df(x, y, x, df)
    return (((y - fit) ** 2) / ((1 - df/n) ** 2)).mean()

def lcv(x, y, df):

    fit = smooth_df(x, y, x, df)
    lev = np.diag(S_df(x, df=df))
    return (((y - fit) ** 2) / ((1 - lev) ** 2)).mean()

def os_srch(span_values,x,y):
  output = []
  for i,span in enumerate(span_values):
    df = sm_diag(x,span).sum()
    gcv_output = gcv(x, y, df)
    lcv_output = lcv(x, y, df)
    output.append([i,span, gcv_output, lcv_output])

    print(f"i={i};span={span}; df={df};  gcv_output={gcv_output};" +f"lcv_output={lcv_output}\n")

  output_pd = pd.DataFrame(output, columns=['I', 'Span','GCV','LCV'])

  min_gcv = output_pd ['GCV'].idxmin()
  min_lcv = output_pd ['LCV'].idxmin()

  min_gcv_ss = output_pd.loc[min_gcv, 'GCV']
  min_lcv_ss = output_pd.loc[min_lcv, 'LCV']

  min_gcv_span = output_pd.loc[min_gcv, 'Span']
  min_lcv_span = output_pd.loc[min_lcv, 'Span']

  print(f"when span={min_gcv_span} reach minimium gcv={min_gcv_ss};\n")
  print(f"when span={min_lcv_span} reach minimium lcv={min_lcv_ss};\n")

  return output_pd,min_gcv_span,min_gcv_span

"""Task3. Test your code"""

'''
both LCV and GCV returned the same best span = 0.45
'''

span_values = np.around(np.linspace(0.2, 0.9, 15),2)
x = coding3_data['x'].values
y = coding3_data['y'].values
os_srch_output = os_srch(span_values,x,y)

output_pd,min_gcv_span,min_gcv_span=os_srch_output

plt.plot(output_pd.index, output_pd['GCV'], label='GCV', marker='o')
plt.plot(output_pd.index, output_pd['LCV'], label='LCV', marker='s')
'''
double check the trend of LCV and GCV - pretty consistent for this dataset
'''

"""- Fit a LOESS model over the entire dataset using the selected optimal span.

"""

best_span = 0.45

def f(x):
    return np.sin(12 * (x + 0.2)) / (x + 0.2)

err = 1
x = coding3_data['x'].values
y = coding3_data['y'].values
fx = np.arange(1, 51, 1)/50
fy = f(fx)

lowess_model = loess(x=x, y=y, span=0.45) #use x to fill in y because y is irrelevant
lowess_model.fit()
py = lowess_model.predict(x)

"""Display a chart"""

px2 = np.arange(1, 51, 0.5)/50
py2_1= f(px2)+ np.random.normal(0, err, 100)
lowess_model2 = loess(x=px2, y=py2_1, span=0.45) #use x to fill in y because y is irrelevant
py2 = lowess_model2.predict(px2)

sns.set()
mpl.rcParams['figure.dpi'] = 100
plt.scatter(x, y, color="red", s=2)
plt.plot(fx, fy, color="black", linewidth=1, label='True Function')
plt.plot(x, py, color="blue", linewidth=3, label='Stimulation-original data')
plt.plot(px2, py2, color="blue", linewidth=0.5, label='Stimulation-finer grid')
plt.xlabel("x")
plt.ylabel("y")
plt.legend()

'''
additional comment:
in the chart displayed, we found there is a striaght line between two dots around the middle.
after investigation, we believed due to the sparsity of datapoints around the center
of the chart, so the smoother matrix approximize a straight line between two dots.
to prove our intuition, we add a finer grid line with more dots that spread out more
evenly to the matrix we generated, and the shape of the finer-grid stimuation is more
similar to the true function, and proved our answer is correct.
'''

"""
# **Part II: Ridgeless and Double Descent** By Zilal"""

#initialize seed
np.random.seed(3113)

coding3_dataH = pd.read_csv("https://liangfgithub.github.io/F22/Coding3_dataH.csv", header=None)
coding3_dataH.head()

coding3_dataH.shape

"""**Task** 1: Ridgeless Function"""

def ridgeless(train_data, test_data, eps=1e-10):
    # Step 1: Separate response Y and features X from the training and test datasets
    Y_train = train_data[:, 0]  # First column is Y for training
    X_train = train_data[:, 1:]  # Remaining columns are X for training

    Y_test = test_data[:, 0]  # First column is Y for test
    X_test = test_data[:, 1:]  # Remaining columns are X for test

    # Step 2: Center the training and test data (subtract the mean of each feature)
    X_train_mean = np.mean(X_train, axis=0)  # Mean of training features
    X_train_centered = X_train - X_train_mean  # Center the training data

    #reference: https://campuswire.com/c/GB46E5679/feed
    X_test_centered = X_test - X_train_mean  # Center the test data using the mean of training data

    # Step 3: Estimate the intercept (b0) as the mean of Y_train
    b0 = np.mean(Y_train)

    # Step 4: Perform Singular Value Decomposition (SVD) on the centered training data X_train
    U_train, S_train, Vt_train = np.linalg.svd(X_train_centered, full_matrices=True)

    # Step 5: Identify the number of singular values greater than eps, and truncate
    k = np.sum(S_train > eps)  # Number of significant singular values
    S_train_trunc = S_train[:k]  # Truncated singular values
    Vt_train_trunc = Vt_train[:k, :]  # Truncated right singular vectors (V^T matrix)

    # Step 6: Transform the centered training data into the truncated principal component space
    F_train = X_train_centered @ Vt_train_trunc.T

    # Step 7: Compute the least squares coefficients alpha
    # Since F.T @ F is diagonal, we can compute alpha without matrix inversion:
    # alpha = (U'Y) / S
    alpha = (U_train[:, :k].T @ Y_train) / S_train_trunc

    # Step 8: Compute predictions for the training data
    Y_train_pred = b0 + F_train @ alpha  # Training predictions: y_train_pred = b0 + F_train @ alpha

    # Step 9: Calculate the training error (Mean Squared Error)
    train_error = np.mean((Y_train - Y_train_pred) ** 2)

    # Step 10: Transform the centered test data into the truncated principal component space
    F_test = X_test_centered @ Vt_train_trunc.T  # F_test = X_test_centered * V_trunc

    # Step 11: Compute predictions for the test data
    Y_test_pred = b0 + F_test @ alpha

    # Step 12: Calculate the test error (Mean Squared Error)
    test_error = np.mean((Y_test - Y_test_pred) ** 2)

    # Return training and test errors
    return train_error, test_error

"""reference: https://campuswire.com/c/GB46E5679/feed/338

### **Task 2: Simulation study** By Zilal

Graphical display
"""

def execute_ridgeless_procedure(data, T=30):
    # Initialize the matrix to store test errors, with dimensions 30 (iterations) x 236 (parameters)
    test_error_matrix = np.zeros((T, 236))

    # Loop over the number of iterations
    for t in range(T):
        # Partition the data into 25% training and 75% testing
        # random state within the loop have to be None or ignore: https://campuswire.com/c/GB46E5679/feed
        train_data, test_data = train_test_split(data, test_size=0.75, random_state=None)

        # Loop over the number of features d = 6 to 241
        for d in range(6, 242):
            # Extract the first 'd' columns (after the first Y column) of the train and test data
            train_subset = np.hstack((train_data[:, [0]], train_data[:, 1:d]))
            test_subset = np.hstack((test_data[:, [0]], test_data[:, 1:d]))

            # Calculate the test error using the ridgeless function
            _, test_error = ridgeless(train_subset, test_subset)

            # Store the log of the test error in the matrix
            test_error_matrix[t, d-6] = np.log(test_error)

    # Compute the median of the test errors across the 30 iterations for each parameter count
    median_test_errors = np.median(test_error_matrix, axis=0)

    # Plot the median test errors (in log scale) against the number of regression parameters (5 to 240)
    plt.figure(figsize=(10, 4))
    plt.plot(range(5, 241), median_test_errors, marker='o', color='b',linestyle='None', markerfacecolor='None',  markeredgecolor='b')
    plt.xticks(range(0, 241, 50))  # Set x-axis ticks at 50-unit intervals
    plt.yticks(range(-2, 7, 2))  # Set y-axis ticks at 2-unit intervals


    plt.xlabel('# of Features')
    plt.ylabel('Log of TestError')
    plt.title('Median Log Test Error vs Number of Regression Parameters')
    plt.show()

execute_ridgeless_procedure(coding3_dataH.to_numpy())

"""# **Part III: Clustering time series**"""

#initialize seed
np.random.seed(3113)

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler


#np.random.seed(7981)


# Load and center data
data_path = 'https://liangfgithub.github.io/Data/Sales_Transactions_Dataset_Weekly.csv'
df = pd.read_csv(data_path)
weekly_cols = [f'W{i}' for i in range(52)]
X = df[weekly_cols].values
X_centered = X - X.mean(axis=1, keepdims=True)

# Define knots and generate design matrix F for natural cubic splines
x = np.arange(1, 53).reshape(-1, 1)
knots = np.linspace(x.min(), x.max(), 10)[1:-1]

def hinge_basis(x, knots):
    F = [x.flatten()]
    for knot in knots:
        F.append(np.maximum(x.flatten() - knot, 0)**3)
    return np.column_stack(F)

F = hinge_basis(x, knots)
F_centered = F - F.mean(axis=0)

# Compute coefficients matrix B
FtF_inv = np.linalg.pinv(F_centered.T @ F_centered)
FtX = F_centered.T @ X_centered.T
B = (FtF_inv @ FtX).T

# K-means clustering
k = 6
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(B)
df['Cluster'] = labels

# Reconstruct cluster centers and plot
centers_B = kmeans.cluster_centers_
centers_ts = F_centered @ centers_B.T

fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharex=True, sharey=True)
axes = axes.flatten()

for i, ax in enumerate(axes):
    if i < k:
        cluster_data = X_centered[labels == i]
        for ts in cluster_data:
            ax.plot(ts, color='grey', alpha=0.5)
        ax.plot(centers_ts[:, i], color='red', linewidth=2)
        ax.set_title(f'Cluster {i+1}')
        ax.set_xlabel('Week')
        ax.set_ylabel('Centered Sales')
    else:
        ax.axis('off')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.suptitle('K-Means Clustering of Time Series Data (6 Clusters)', fontsize=16)
plt.show()

"""###part 3 By Zilal

##Reference:
- Part 3(NCS): https://liangfgithub.github.io/Python_W5_RegressionSpline.html

To inspect the basis functions of a natural cubic spline, utilize the ns command. There are 10 basis functions for NCS with 10 knots (8 interior knots and 2 boundary knots).
"""

from scipy.interpolate import splev, interp1d
#initialize seed
np.random.seed(3113)

 #converted from R's ns()
def ns(x, df=None, knots=None, boundary_knots=None, include_intercept=False):
    degree = 3

    if boundary_knots is None:
        boundary_knots = [np.min(x), np.max(x)]
    else:
        boundary_knots = np.sort(boundary_knots).tolist()

    oleft = x < boundary_knots[0]
    oright = x > boundary_knots[1]
    outside = oleft | oright
    inside = ~outside

    if df is not None:
        nIknots = df - 1 - include_intercept
        if nIknots < 0:
            nIknots = 0

        if nIknots > 0:
            knots = np.linspace(0, 1, num=nIknots + 2)[1:-1]
            knots = np.quantile(x[~outside], knots)

    Aknots = np.sort(np.concatenate((boundary_knots * 4, knots)))
    n_bases = len(Aknots) - (degree + 1)

    if any(outside):
        basis = np.empty((x.shape[0], n_bases), dtype=float)
        e = 1 / 4 # in theory anything in (0, 1); was (implicitly) 0 in R <= 3.2.2

        if any(oleft):
            k_pivot = boundary_knots[0]
            xl = x[oleft] - k_pivot
            xl = np.c_[np.ones(xl.shape[0]), xl]

            # equivalent to splineDesign(Aknots, rep(k.pivot, ord), ord, derivs)
            tt = np.empty((xl.shape[1], n_bases), dtype=float)
            for j in range(xl.shape[1]):
                for i in range(n_bases):
                    coefs = np.zeros((n_bases,))
                    coefs[i] = 1
                    tt[j, i] = splev(k_pivot, (Aknots, coefs, degree), der=j)

            basis[oleft, :] = xl @ tt

        if any(oright):
            k_pivot = boundary_knots[1]
            xr = x[oright] - k_pivot
            xr = np.c_[np.ones(xr.shape[0]), xr]

            tt = np.empty((xr.shape[1], n_bases), dtype=float)
            for j in range(xr.shape[1]):
                for i in range(n_bases):
                    coefs = np.zeros((n_bases,))
                    coefs[i] = 1
                    tt[j, i] = splev(k_pivot, (Aknots, coefs, degree), der=j)

            basis[oright, :] = xr @ tt

        if any(inside):
            xi = x[inside]
            tt = np.empty((len(xi), n_bases), dtype=float)
            for i in range(n_bases):
                coefs = np.zeros((n_bases,))
                coefs[i] = 1
                tt[:, i] = splev(xi, (Aknots, coefs, degree))

            basis[inside, :] = tt
    else:
        basis = np.empty((x.shape[0], n_bases), dtype=float)
        for i in range(n_bases):
            coefs = np.zeros((n_bases,))
            coefs[i] = 1
            basis[:, i] = splev(x, (Aknots, coefs, degree))

    const = np.empty((2, n_bases), dtype=float)
    for i in range(n_bases):
        coefs = np.zeros((n_bases,))
        coefs[i] = 1
        const[:, i] = splev(boundary_knots, (Aknots, coefs, degree), der=2)

    if include_intercept is False:
        basis = basis[:, 1:]
        const = const[:, 1:]

    qr_const = np.linalg.qr(const.T, mode='complete')[0]
    basis = (qr_const.T @ basis.T).T[:, 2:]

    return basis

from scipy.linalg import pinv

warnings.filterwarnings('ignore')

data_path = 'https://liangfgithub.github.io/Data/Sales_Transactions_Dataset_Weekly.csv'
df = pd.read_csv(data_path)

# Extract and center weekly sales
weekly_cols = [f'W{i}' for i in range(52)]
X = df[weekly_cols].values
X_centered = X - X.mean(axis=1, keepdims=True)


# Define the index as the feature (1 to 52)
index = np.arange(1, 53)

# Initialize the matrix to store the NCS coefficients
B = np.zeros((811, 9))  # 811 products, 9 coefficients

# Fit NCS for each time series (each row of X)
for i in range(X_centered.shape[0]):
    # Create the design matrix F (52x9)
    F = ns(index, df=9, include_intercept=False)

    # Center the design matrix by removing the column means
    F_centered = F - np.mean(F, axis=0)
    # Initialize matrix B to store coefficients
    B = np.zeros((811, 9))  # 811 products, 9 coefficients
    B=(np.linalg.inv(F_centered.T @ F_centered) @ F_centered.T @ X_centered.T).T


print((B.shape))

# Run K-Means clustering on B
num_clusters = 6
kmeans = KMeans(n_clusters=num_clusters, random_state=None)
kmeans.fit(B)
# Get the cluster labels for each product
labels = kmeans.labels_

# Calculate the cluster centers in B space
cluster_centers_b = kmeans.cluster_centers_

# Visualize Centered Time Series with Cluster Matrix B
# Set up the plotting
fig, axs = plt.subplots(2, 3, figsize=(15, 10))
axs = axs.flatten()

# Plot each cluster
for cluster_idx in range(num_clusters):
    # Get indices of products in the current cluster
    idx = np.where(labels == cluster_idx)[0]

    # Plot each time series in the cluster
    for i in idx:
        axs[cluster_idx].plot(X_centered[i], color='grey', alpha=0.5)  # Centered time series in grey

    # Calculate the corresponding time series from the cluster center b
    cluster_time_series = F_centered @ cluster_centers_b[cluster_idx] # Matrix product to get the corresponding time series

    # Plot the cluster center in red
    axs[cluster_idx].plot(cluster_time_series, color='red', linewidth=2, label='Cluster Center')

    # Add title and legend
    axs[cluster_idx].set_title(f'Cluster {cluster_idx + 1}')
    axs[cluster_idx].legend()


plt.suptitle('K-Means Clustering of Time Series Data Matrix B', fontsize=16)
plt.show()

# Run K-Means clustering on X
kmeans.fit(X_centered)
# Get the cluster labels and cluster centers in X space
labels = kmeans.labels_
centers = kmeans.cluster_centers_

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i in range(num_clusters):
    # Get all products belonging to the current cluster
    cluster_indices = np.where(labels == i)[0]  # Indices of products in the i-th cluster

    # Plot all time series in grey for the i-th cluster
    for idx in cluster_indices:
        axes[i].plot(X_centered[idx], color='grey', alpha=0.5)

    # Overlay the cluster center in red
    axes[i].plot(centers[i], color='red', linewidth=2, label='Cluster Center')
    axes[i].set_title(f'Cluster {i + 1}')
    axes[i].legend()


plt.suptitle('K-Means Clustering of Time Series Data Matrix X', fontsize=16)

plt.show()

"""Task 1: Fitting NCS

Task 2: Cluster Matrix B

Task 3: Cluster Matrix X

Graphical display
"""



"""# **=====================  CODE ARCHIVE ===============================**"""

#Part1- Jianci - just take a look thr ture curve, not relevant to the project
x = np.linspace(-10, 10, 100)
y = np.sin(12*(x+0.2))/(x+0.2)
plt.figure(figsize = (12,8))
plt.plot(x,y)